{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hackernews Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "* Requests: To make http(s) requests\n",
    "* BeautifulSoup4: To \n",
    "* PyMongo: To interact with mongodb database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Extracting the MetaData of all posts from all pages specified by the User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a function that returns the parsed html soup of a link given as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_to_soup(link):\n",
    "    response = requests.get(link)\n",
    "    if response.ok:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requesting to get the home page of TheHackerNews.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_page_soup = link_to_soup('https://thehackernews.com/')\n",
    "print(home_page_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to parse the html document as well. Let's ask the user, for how many pages, the data has to be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_pages = int(input('How many pages to extract the data from? '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse the the rest of the pages and append all of the pages to the array 'pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "pages.append(home_page_soup)\n",
    "No_pages -= 1\n",
    "for i in range(No_pages):\n",
    "    next_page_link = pages[i].find(\"a\", class_=\"blog-pager-older-link-mobile\")['href']\n",
    "    pages.append(link_to_soup(next_page_link))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pages parsed to extract the data from. So, let's extract all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_url_title_data = []\n",
    "posts_url_others_data = []\n",
    "\n",
    "for page in pages:\n",
    "    posts_in_page = home_page_soup.find_all(\"a\", class_='story-link')\n",
    "    for post in posts_in_page:\n",
    "        posts_url_title_data.append({\n",
    "        \"url\" : post['href'],\n",
    "        \"title\" : post.find(\"h2\", class_='home-title').text\n",
    "        })\n",
    "        \n",
    "        posts_url_others_data.append({\n",
    "        \"url\" : post['href'],\n",
    "        \"desc\" : post.find(\"div\", class_='home-desc').text,\n",
    "        \"author\" : post.find(\"span\").text[1:len(post.find(\"span\").text)-1],\n",
    "        \"imgSrc\" : post.find(\"img\")['data-src']\n",
    "        })\n",
    "    \n",
    "print('URL of 1st post: ', posts_url_title_data[0]['url'])\n",
    "print('Description of 1st post: ', posts_url_others_data[0]['desc'])\n",
    "print('Author of the 1st post: ', posts_url_others_data[0]['author'])\n",
    "print('Image source of the 1st post: ', posts_url_others_data[0]['imgSrc'])\n",
    "print('Title of the 1st post: ', posts_url_title_data[0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata of all posts from all pages is extracted. Phase 1 completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2a:  Store the data in a MongoDb database\n",
    "\n",
    "We'll store the data in two documents having:\n",
    "1. Url and title of the blog\n",
    "2. Url and Desription, Image, Author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Let's connect to the mongodb server by asking the user MongoDbURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoDbURI = input('Enter the mongoDbURI: ')\n",
    "client = MongoClient(mongoDbURI) # for users running mongo server on local machine: 'mongodb://localhost:27017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the database\n",
    "db = client['hackernews']\n",
    "\n",
    "# Getting the collections\n",
    "urlTitle = db['url-title']\n",
    "urlOthers = db['url-others']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection done, now we'll insert the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = urlTitle.insert_many(posts_url_title_data)\n",
    "result2 = urlOthers.insert_many(posts_url_others_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the values were inserted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1.inserted_ids[:5])\n",
    "print(result2.inserted_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the Object Ids of the inserted documents, that means the documents were inserted to the database successfully.\n",
    "\n",
    "Phase 2a completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2b: Save the data as JSON\n",
    "\n",
    "We will save the data as a single JSON file, merging `posts_url_title_data` with `posts_url_others_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data into a single dictionary\n",
    "n = len(posts_url_title_data)\n",
    "posts_url_data = [ dict(posts_url_title_data[0], **posts_url_others_data[0]) for i in range(n) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = input(\"Enter the location to save the JSON to: \")\n",
    "output_path = Path(output_location)\n",
    "\n",
    "# Create any necessary parent directories\n",
    "if not output_path.parent.exists():\n",
    "    print(\"Directory {} does not exist, creating.\".format(output_path.parent))\n",
    "    output_path.parent.mkdir(parents=True)\n",
    "\n",
    "# Save JSON\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    print(\"Saving to {}.\".format(output_path))\n",
    "    output_file.write(json.dumps(posts_url_data))\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
